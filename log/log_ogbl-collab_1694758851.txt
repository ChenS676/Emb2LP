Namespace(device=7, encoder='HLGNN', predictor='DOT', optimizer='Adam', loss_func='AdaAUC', neg_sampler='global', data_name='ogbl-collab', data_path='dataset', eval_metric='hits', walk_start_type='edge', res_dir='log', pretrain_emb=None, gnn_num_layers=15, mlp_num_layers=2, emb_hidden_channels=256, gnn_hidden_channels=256, mlp_hidden_channels=256, dropout=0.3, grad_clip_norm=2.0, batch_size=65536, lr=0.001, num_neg=1, walk_length=5, epochs=800, log_steps=1, eval_steps=5, runs=1, year=2010, use_lr_decay=False, use_node_feats=True, use_coalesce=False, train_node_emb=True, use_valedges_as_input=True, eval_last_best=True, random_walk_augment=False, alpha=0.5, init='NPPR')
Total number of model parameters is 60480784
Hits@20
Run: 01, Epoch: 05, Loss: 408.9793, Learning Rate: 0.0010, Valid: 1.72%, Test: 1.20%
Hits@50
Run: 01, Epoch: 05, Loss: 408.9793, Learning Rate: 0.0010, Valid: 2.75%, Test: 2.13%
Hits@100
Run: 01, Epoch: 05, Loss: 408.9793, Learning Rate: 0.0010, Valid: 4.11%, Test: 3.11%
Hits@20
Run: 01, Epoch: 10, Loss: 229.5305, Learning Rate: 0.0010, Valid: 0.16%, Test: 0.20%
Hits@50
Run: 01, Epoch: 10, Loss: 229.5305, Learning Rate: 0.0010, Valid: 0.27%, Test: 0.39%
Hits@100
Run: 01, Epoch: 10, Loss: 229.5305, Learning Rate: 0.0010, Valid: 0.40%, Test: 0.62%
Hits@20
Run: 01, Epoch: 15, Loss: 143.9460, Learning Rate: 0.0010, Valid: 0.20%, Test: 0.27%
Hits@50
Run: 01, Epoch: 15, Loss: 143.9460, Learning Rate: 0.0010, Valid: 0.39%, Test: 0.52%
Hits@100
Run: 01, Epoch: 15, Loss: 143.9460, Learning Rate: 0.0010, Valid: 0.73%, Test: 0.96%
Hits@20
Run: 01, Epoch: 20, Loss: 107.9864, Learning Rate: 0.0010, Valid: 0.38%, Test: 0.46%
Hits@50
Run: 01, Epoch: 20, Loss: 107.9864, Learning Rate: 0.0010, Valid: 0.75%, Test: 0.90%
Hits@100
Run: 01, Epoch: 20, Loss: 107.9864, Learning Rate: 0.0010, Valid: 1.54%, Test: 1.67%
Hits@20
Run: 01, Epoch: 25, Loss: 87.8933, Learning Rate: 0.0010, Valid: 0.44%, Test: 0.56%
Hits@50
Run: 01, Epoch: 25, Loss: 87.8933, Learning Rate: 0.0010, Valid: 0.92%, Test: 1.11%
Hits@100
Run: 01, Epoch: 25, Loss: 87.8933, Learning Rate: 0.0010, Valid: 2.01%, Test: 2.04%
Hits@20
Run: 01, Epoch: 30, Loss: 75.0865, Learning Rate: 0.0010, Valid: 0.59%, Test: 0.70%
Hits@50
Run: 01, Epoch: 30, Loss: 75.0865, Learning Rate: 0.0010, Valid: 1.48%, Test: 1.54%
Hits@100
Run: 01, Epoch: 30, Loss: 75.0865, Learning Rate: 0.0010, Valid: 2.81%, Test: 2.58%
Hits@20
Run: 01, Epoch: 35, Loss: 66.6080, Learning Rate: 0.0010, Valid: 0.71%, Test: 0.80%
Hits@50
Run: 01, Epoch: 35, Loss: 66.6080, Learning Rate: 0.0010, Valid: 2.12%, Test: 2.01%
Hits@100
Run: 01, Epoch: 35, Loss: 66.6080, Learning Rate: 0.0010, Valid: 3.74%, Test: 3.08%
Hits@20
Run: 01, Epoch: 40, Loss: 60.4390, Learning Rate: 0.0010, Valid: 0.96%, Test: 1.06%
Hits@50
Run: 01, Epoch: 40, Loss: 60.4390, Learning Rate: 0.0010, Valid: 2.32%, Test: 2.12%
Hits@100
Run: 01, Epoch: 40, Loss: 60.4390, Learning Rate: 0.0010, Valid: 4.70%, Test: 3.68%
Hits@20
Run: 01, Epoch: 45, Loss: 55.0885, Learning Rate: 0.0010, Valid: 1.44%, Test: 1.41%
Hits@50
Run: 01, Epoch: 45, Loss: 55.0885, Learning Rate: 0.0010, Valid: 2.82%, Test: 2.45%
Hits@100
Run: 01, Epoch: 45, Loss: 55.0885, Learning Rate: 0.0010, Valid: 5.33%, Test: 3.97%
